{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0Nguyen0Cong0Tuan0/Road-Buddy-Challenge/blob/main/models/yolo_finetune_both.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-section"
      },
      "source": [
        "# **YOLO11 Unified Dataset Training: Road Lane + BDD100K**\n",
        "\n",
        "This notebook trains YOLO models on a **unified dataset** combining:\n",
        "- **Road Lane v2** (6 lane type classes)\n",
        "- **BDD100K** (11 traffic classes, excluding generic 'lane')\n",
        "\n",
        "> **Note**: This approach prevents catastrophic forgetting by training on all 17 classes simultaneously.\n",
        "\n",
        "**Unified Classes (17 total)**\n",
        "\n",
        "| ID | Class | Source |\n",
        "|----|-------|--------|\n",
        "| 0-5 | Lane types (divider, dotted, double, random, road-sign, solid) | Road Lane |\n",
        "| 6-16 | Traffic objects (bike, bus, car, drivable area, motor, person, rider, traffic light, traffic sign, train, truck) | BDD100K |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## **Setup & Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn9CRB0Ewy2C"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-ultralytics"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "import seaborn as sns\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {DEVICE}')\n",
        "if DEVICE == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Dataset Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AUCINCGw0RU"
      },
      "outputs": [],
      "source": [
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'models' else Path.cwd()\n",
        "DATA_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'traffic datasets'\n",
        "MODELS_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'models'\n",
        "RUNS_DIR = PROJECT_ROOT / 'drive' / 'MyDrive' / 'runs'\n",
        "\n",
        "# Unified dataset path\n",
        "UNIFIED_DATASET = {\n",
        "    'path': DATA_DIR / 'unified',\n",
        "    'yaml': DATA_DIR / 'unified' / 'data_unified.yaml',\n",
        "    'description': 'Unified Road Lane + BDD100K dataset (17 classes)',\n",
        "    'classes': [\n",
        "        # Road Lane classes (0-5)\n",
        "        'divider-line', 'dotted-line', 'double-line',\n",
        "        'random-line', 'road-sign-line', 'solid-line',\n",
        "        # BDD100K classes (6-16)\n",
        "        'bike', 'bus', 'car', 'drivable area', 'motor',\n",
        "        'person', 'rider', 'traffic light', 'traffic sign', 'train', 'truck'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Class grouping for analysis\n",
        "LANE_CLASSES = list(range(0, 6))   # Classes 0-5\n",
        "TRAFFIC_CLASSES = list(range(6, 17))  # Classes 6-16\n",
        "\n",
        "# Verify dataset\n",
        "if UNIFIED_DATASET['yaml'].exists():\n",
        "    print(f\"Unified dataset found: {UNIFIED_DATASET['path']}\")\n",
        "    with open(UNIFIED_DATASET['yaml'], 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "        print(f\"  Classes: {config['nc']}\")\n",
        "        print(f\"  Lane types: {len(LANE_CLASSES)} classes\")\n",
        "        print(f\"  Traffic objects: {len(TRAFFIC_CLASSES)} classes\")\n",
        "else:\n",
        "    print(f\"Unified dataset NOT found at {UNIFIED_DATASET['yaml']}\")\n",
        "    print(\"Please run the merge_datasets.py script first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Enhanced Training Configuration**\n",
        "\n",
        "Optimized training settings with:\n",
        "- **100 epochs** for better convergence\n",
        "- **Cosine learning rate scheduler** for smooth decay\n",
        "- **Warmup epochs** to stabilize early training\n",
        "- **Data augmentation** for robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzPJ-DGy5Dr9"
      },
      "outputs": [],
      "source": [
        "# Enhanced training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    # Core settings\n",
        "    'epochs': 5,             # More epochs for better convergence\n",
        "    'imgsz': 640,            # Image size\n",
        "    'batch': 16,             # Batch size (reduce if GPU OOM)\n",
        "    'patience': 20,          # Early stopping patience\n",
        "    'device': DEVICE,        # Training device\n",
        "    'workers': 4,            # Data loader workers\n",
        "    \n",
        "    # Learning rate settings\n",
        "    'lr0': 0.01,             # Initial learning rate\n",
        "    'lrf': 0.01,             # Final learning rate (lr0 * lrf)\n",
        "    'warmup_epochs': 3,      # Warmup epochs for stable start\n",
        "    'warmup_momentum': 0.8,  # Warmup initial momentum\n",
        "    'warmup_bias_lr': 0.1,   # Warmup initial bias lr\n",
        "    'cos_lr': True,          # Use cosine learning rate scheduler\n",
        "    \n",
        "    # Optimizer settings\n",
        "    'optimizer': 'AdamW',    # AdamW optimizer (better than SGD for fine-tuning)\n",
        "    'momentum': 0.937,       # SGD momentum/Adam beta1\n",
        "    'weight_decay': 0.0005,  # Optimizer weight decay\n",
        "    \n",
        "    # Data augmentation\n",
        "    'hsv_h': 0.015,          # HSV-Hue augmentation\n",
        "    'hsv_s': 0.7,            # HSV-Saturation augmentation\n",
        "    'hsv_v': 0.4,            # HSV-Value augmentation\n",
        "    'degrees': 0.0,          # Rotation augmentation (disabled for driving scenes)\n",
        "    'translate': 0.1,        # Translation augmentation\n",
        "    'scale': 0.5,            # Scale augmentation\n",
        "    'shear': 0.0,            # Shear augmentation (disabled)\n",
        "    'flipud': 0.0,           # Vertical flip (disabled - unnatural for driving)\n",
        "    'fliplr': 0.5,           # Horizontal flip\n",
        "    'mosaic': 1.0,           # Mosaic augmentation\n",
        "    'mixup': 0.1,            # Mixup augmentation\n",
        "    \n",
        "    # Output settings\n",
        "    'save': True,            # Save checkpoints\n",
        "    'save_period': 10,       # Save checkpoint every N epochs\n",
        "    'plots': True,           # Generate training plots\n",
        "    'verbose': True,         # Verbose output\n",
        "}\n",
        "\n",
        "print(\"\\nCore Settings:\")\n",
        "for key in ['epochs', 'imgsz', 'batch', 'patience', 'device']:\n",
        "    print(f\"   {key}: {TRAINING_CONFIG[key]}\")\n",
        "\n",
        "print(\"\\nLearning Rate:\")\n",
        "for key in ['lr0', 'lrf', 'warmup_epochs', 'cos_lr', 'optimizer']:\n",
        "    print(f\"   {key}: {TRAINING_CONFIG[key]}\")\n",
        "\n",
        "print(\"\\nData Augmentation:\")\n",
        "for key in ['mosaic', 'mixup', 'fliplr', 'scale', 'translate']:\n",
        "    print(f\"   {key}: {TRAINING_CONFIG[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Train YOLO11n on Unified Dataset**\n",
        "\n",
        "Training from pretrained YOLO11n model on the unified dataset with 17 classes.\n",
        "This ensures the model can detect both lane types AND traffic objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fresh pretrained YOLO11n (NOT Road Lane fine-tuned)\n",
        "print('Loading pretrained YOLO11n...')\n",
        "model_n = YOLO('yolo11n.pt')\n",
        "model_n.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train YOLO11n on unified dataset\n",
        "print('Training YOLO11n on Unified Dataset (17 classes)')\n",
        "print('This model will detect BOTH lane types AND traffic objects!')\n",
        "print(f'\\nUsing cosine LR scheduler: {TRAINING_CONFIG[\"lr0\"]} -> {TRAINING_CONFIG[\"lr0\"] * TRAINING_CONFIG[\"lrf\"]}')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results_n = model_n.train(\n",
        "    data=str(UNIFIED_DATASET['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11n_unified',\n",
        "    exist_ok=True,\n",
        "    **TRAINING_CONFIG\n",
        ")\n",
        "\n",
        "training_time_n = time.time() - start_time\n",
        "print(f'\\n YOLO11n training completed in {training_time_n/3600:.2f} hours')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Train YOLO11l on Unified Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fresh pretrained YOLO11l (NOT Road Lane fine-tuned)\n",
        "print('Loading pretrained YOLO11l...')\n",
        "model_l = YOLO('yolo11l.pt')\n",
        "model_l.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train YOLO11l on unified dataset (reduced batch size for larger model)\n",
        "print('Training YOLO11l on Unified Dataset (17 classes)')\n",
        "\n",
        "config_l = TRAINING_CONFIG.copy()\n",
        "config_l['batch'] = 8  # Reduced batch size for larger model\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results_l = model_l.train(\n",
        "    data=str(UNIFIED_DATASET['yaml']),\n",
        "    project=str(RUNS_DIR / 'finetune'),\n",
        "    name='yolo11l_unified',\n",
        "    exist_ok=True,\n",
        "    **config_l\n",
        ")\n",
        "\n",
        "training_time_l = time.time() - start_time\n",
        "print(f'\\n YOLO11l training completed in {training_time_l/3600:.2f} hours')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Comprehensive Validation**\n",
        "\n",
        "Detailed validation metrics including:\n",
        "- **Overall metrics**: mAP50, mAP50-95, Precision, Recall, F1\n",
        "- **Per-class performance**: Separate analysis for lane types vs traffic objects\n",
        "- **Confusion matrix**: Visualize misclassifications\n",
        "- **Performance comparison**: YOLO11n vs YOLO11l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(runs_dir, model_name):\n",
        "    \"\"\"Load a trained model from the runs directory.\"\"\"\n",
        "    model_path = runs_dir / 'finetune' / model_name / 'weights' / 'best.pt'\n",
        "    if model_path.exists():\n",
        "        return YOLO(str(model_path))\n",
        "    else:\n",
        "        print(f\"Model not found: {model_path}\")\n",
        "        return None\n",
        "\n",
        "# Load trained models\n",
        "print('Loading trained models...')\n",
        "model_n_trained = load_trained_model(RUNS_DIR, 'yolo11n_unified')\n",
        "model_l_trained = load_trained_model(RUNS_DIR, 'yolo11l_unified')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset_yaml, model_name, class_names):\n",
        "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "    print(f\"Evaluating {model_name}\")\n",
        "    \n",
        "    # Run validation\n",
        "    metrics = model.val(\n",
        "        data=str(dataset_yaml),\n",
        "        split='test',\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    # Overall metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"   mAP50:      {metrics.box.map50:.4f}\")\n",
        "    print(f\"   mAP50-95:   {metrics.box.map:.4f}\")\n",
        "    print(f\"   Precision:  {metrics.box.mp:.4f}\")\n",
        "    print(f\"   Recall:     {metrics.box.mr:.4f}\")\n",
        "    f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
        "    print(f\"   F1 Score:   {f1:.4f}\")\n",
        "    \n",
        "    # Per-class AP50\n",
        "    ap50_per_class = metrics.box.ap50\n",
        "    \n",
        "    # Lane classes performance\n",
        "    print(\"\\nLane Types Performance:\")\n",
        "    lane_aps = []\n",
        "    for i in LANE_CLASSES:\n",
        "        if i < len(ap50_per_class):\n",
        "            ap = ap50_per_class[i]\n",
        "            lane_aps.append(ap)\n",
        "            print(f\"   {class_names[i]}: {ap:.4f}\")\n",
        "    print(f\"   Average Lane mAP50: {np.mean(lane_aps):.4f}\")\n",
        "    \n",
        "    # Traffic classes performance\n",
        "    print(\"\\nTraffic Objects Performance:\")\n",
        "    traffic_aps = []\n",
        "    for i in TRAFFIC_CLASSES:\n",
        "        if i < len(ap50_per_class):\n",
        "            ap = ap50_per_class[i]\n",
        "            traffic_aps.append(ap)\n",
        "            print(f\"   {class_names[i]}: {ap:.4f}\")\n",
        "    print(f\"   Average Traffic mAP50: {np.mean(traffic_aps):.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'map50': metrics.box.map50,\n",
        "        'map': metrics.box.map,\n",
        "        'precision': metrics.box.mp,\n",
        "        'recall': metrics.box.mr,\n",
        "        'f1': f1,\n",
        "        'lane_map50': np.mean(lane_aps),\n",
        "        'traffic_map50': np.mean(traffic_aps),\n",
        "        'ap50_per_class': ap50_per_class\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both models\n",
        "class_names = UNIFIED_DATASET['classes']\n",
        "results_dict = {}\n",
        "\n",
        "if model_n_trained:\n",
        "    results_dict['yolo11n'] = evaluate_model(\n",
        "        model_n_trained, \n",
        "        UNIFIED_DATASET['yaml'], \n",
        "        'YOLO11n Unified',\n",
        "        class_names\n",
        "    )\n",
        "\n",
        "if model_l_trained:\n",
        "    results_dict['yolo11l'] = evaluate_model(\n",
        "        model_l_trained, \n",
        "        UNIFIED_DATASET['yaml'], \n",
        "        'YOLO11l Unified',\n",
        "        class_names\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison visualization\n",
        "if len(results_dict) >= 2:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Overall metrics comparison\n",
        "    metrics_to_compare = ['map50', 'map', 'precision', 'recall', 'f1']\n",
        "    x = np.arange(len(metrics_to_compare))\n",
        "    width = 0.35\n",
        "    \n",
        "    vals_n = [results_dict['yolo11n'][m] for m in metrics_to_compare]\n",
        "    vals_l = [results_dict['yolo11l'][m] for m in metrics_to_compare]\n",
        "    \n",
        "    axes[0].bar(x - width/2, vals_n, width, label='YOLO11n', color='#3498db')\n",
        "    axes[0].bar(x + width/2, vals_l, width, label='YOLO11l', color='#e74c3c')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(['mAP50', 'mAP50-95', 'Precision', 'Recall', 'F1'])\n",
        "    axes[0].set_ylabel('Score')\n",
        "    axes[0].set_title('Overall Metrics Comparison')\n",
        "    axes[0].legend()\n",
        "    axes[0].set_ylim(0, 1)\n",
        "    \n",
        "    # Lane vs Traffic performance\n",
        "    categories = ['Lane Types', 'Traffic Objects']\n",
        "    x = np.arange(len(categories))\n",
        "    \n",
        "    vals_n = [results_dict['yolo11n']['lane_map50'], results_dict['yolo11n']['traffic_map50']]\n",
        "    vals_l = [results_dict['yolo11l']['lane_map50'], results_dict['yolo11l']['traffic_map50']]\n",
        "    \n",
        "    axes[1].bar(x - width/2, vals_n, width, label='YOLO11n', color='#3498db')\n",
        "    axes[1].bar(x + width/2, vals_l, width, label='YOLO11l', color='#e74c3c')\n",
        "    axes[1].set_xticks(x)\n",
        "    axes[1].set_xticklabels(categories)\n",
        "    axes[1].set_ylabel('mAP50')\n",
        "    axes[1].set_title('Lane vs Traffic Detection')\n",
        "    axes[1].legend()\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    \n",
        "    # Per-class AP50 heatmap\n",
        "    ap_data = np.array([\n",
        "        results_dict['yolo11n']['ap50_per_class'],\n",
        "        results_dict['yolo11l']['ap50_per_class']\n",
        "    ])\n",
        "    \n",
        "    im = axes[2].imshow(ap_data, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    axes[2].set_yticks([0, 1])\n",
        "    axes[2].set_yticklabels(['YOLO11n', 'YOLO11l'])\n",
        "    axes[2].set_xticks(range(len(class_names)))\n",
        "    axes[2].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
        "    axes[2].set_title('Per-Class AP50')\n",
        "    plt.colorbar(im, ax=axes[2], label='AP50')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(str(RUNS_DIR / 'finetune' / 'unified_model_comparison.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n Comparison plot saved to: {RUNS_DIR / 'finetune' / 'unified_model_comparison.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Results Summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "if results_dict:\n",
        "    summary_data = []\n",
        "    for name, res in results_dict.items():\n",
        "        summary_data.append({\n",
        "            'Model': res['model_name'],\n",
        "            'mAP50': f\"{res['map50']:.4f}\",\n",
        "            'mAP50-95': f\"{res['map']:.4f}\",\n",
        "            'Precision': f\"{res['precision']:.4f}\",\n",
        "            'Recall': f\"{res['recall']:.4f}\",\n",
        "            'F1': f\"{res['f1']:.4f}\",\n",
        "            'Lane mAP50': f\"{res['lane_map50']:.4f}\",\n",
        "            'Traffic mAP50': f\"{res['traffic_map50']:.4f}\"\n",
        "        })\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    display(summary_df)\n",
        "    \n",
        "    # Save to CSV\n",
        "    summary_df.to_csv(str(RUNS_DIR / 'finetune' / 'unified_model_summary.csv'), index=False)\n",
        "    print(f\"\\nSummary saved to: {RUNS_DIR / 'finetune' / 'unified_model_summary.csv'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Visual Inference Test**\n",
        "\n",
        "Test the model on sample images to verify it detects both lane types and traffic objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test inference on sample images from both sources\n",
        "test_images_dir = UNIFIED_DATASET['path'] / 'test' / 'images'\n",
        "\n",
        "if test_images_dir.exists() and model_n_trained:\n",
        "    # Get samples from both Road Lane and BDD100K\n",
        "    all_images = list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png'))\n",
        "    \n",
        "    # Try to get mix of both sources\n",
        "    road_lane_images = [p for p in all_images if 'roadlane' in p.name.lower()][:3]\n",
        "    bdd100k_images = [p for p in all_images if 'bdd100k' in p.name.lower()][:3]\n",
        "    sample_images = road_lane_images + bdd100k_images\n",
        "    \n",
        "    if not sample_images:\n",
        "        sample_images = all_images[:6]\n",
        "    \n",
        "    if sample_images:\n",
        "        print(f\"Running inference on {len(sample_images)} test images...\\n\")\n",
        "        \n",
        "        n_images = len(sample_images)\n",
        "        cols = 3\n",
        "        rows = (n_images + cols - 1) // cols\n",
        "        \n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
        "        axes = axes.flatten() if n_images > cols else [axes] if n_images == 1 else axes\n",
        "        \n",
        "        for idx, img_path in enumerate(sample_images):\n",
        "            results = model_n_trained.predict(str(img_path), verbose=False, conf=0.25)\n",
        "            \n",
        "            # Get annotated image\n",
        "            annotated = results[0].plot()\n",
        "            annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            axes[idx].imshow(annotated)\n",
        "            \n",
        "            # Determine source\n",
        "            source = \"Road Lane\" if 'roadlane' in img_path.name.lower() else \"BDD100K\"\n",
        "            axes[idx].set_title(f\"[{source}] {img_path.name[:30]}...\", fontsize=9)\n",
        "            axes[idx].axis('off')\n",
        "            \n",
        "            # Count detections by category\n",
        "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
        "                detected_classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "                lane_count = sum(1 for c in detected_classes if c in LANE_CLASSES)\n",
        "                traffic_count = sum(1 for c in detected_classes if c in TRAFFIC_CLASSES)\n",
        "                print(f\"{img_path.name}: Lanes: {lane_count}, Traffic: {traffic_count}\")\n",
        "            else:\n",
        "                print(f\"{img_path.name}: No detections\")\n",
        "        \n",
        "        # Hide empty subplots\n",
        "        for idx in range(len(sample_images), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(str(RUNS_DIR / 'finetune' / 'unified_inference_samples.png'), dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No test images found.\")\n",
        "else:\n",
        "    print(f\"Test directory not found or model not loaded.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
