{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0Nguyen0Cong0Tuan0/Road-Buddy-Challenge/blob/main/models/yolo_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title-section"
      },
      "source": [
        "# **YOLO11 Fine-tuned Models Benchmark**\n",
        "\n",
        "This notebook **benchmarks and evaluates** all 6 fine-tuned YOLO models:\n",
        "\n",
        "| Model | Training | Description |\n",
        "|-------|----------|-------------|\n",
        "| yolo11n_road_lane | Road Lane only | Nano model for lane detection |\n",
        "| yolo11l_road_lane | Road Lane only | Large model for lane detection |\n",
        "| yolo11n_bdd100k | BDD100K only | Nano model for traffic objects |\n",
        "| yolo11l_bdd100k | BDD100K only | Large model for traffic objects |\n",
        "| yolo11n_both | Road Lane -> BDD100K | Nano model (sequential training) |\n",
        "| yolo11l_both | Road Lane -> BDD100K | Large model (sequential training) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## **Setup & Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn9CRB0Ewy2C"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-ultralytics"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AUCINCGw0RU"
      },
      "outputs": [],
      "source": [
        "# Dataset paths (Kaggle input datasets)\n",
        "DATA_DIR = Path('/kaggle/input/custom-train-data')\n",
        "\n",
        "# Fine-tuned models path\n",
        "MODELS_DIR = Path('/kaggle/input/finetune-model')\n",
        "\n",
        "# Dataset configurations\n",
        "DATASETS = {\n",
        "    'bdd100k': {\n",
        "        'path': DATA_DIR / 'bdd100k',\n",
        "        'yaml': DATA_DIR / 'bdd100k' / 'data_yolo.yaml',\n",
        "        'description': 'BDD100K - Vehicles, pedestrians, traffic objects',\n",
        "        'classes': ['bike', 'bus', 'car', 'drivable area', 'lane', 'motor',\n",
        "                   'person', 'rider', 'traffic light', 'traffic sign', 'train', 'truck']\n",
        "    },\n",
        "    'road_lane': {\n",
        "        'path': DATA_DIR / 'Road Lane.v2i.yolo26',\n",
        "        'yaml': DATA_DIR / 'Road Lane.v2i.yolo26' / 'data.yaml',\n",
        "        'description': 'Road Lane v2 - Lane line types',\n",
        "        'classes': ['divider-line', 'dotted-line', 'double-line',\n",
        "                   'random-line', 'road-sign-line', 'solid-line']\n",
        "    }\n",
        "}\n",
        "\n",
        "for name, info in DATASETS.items():\n",
        "    exists = info['path'].exists()\n",
        "    yaml_exists = info['yaml'].exists()\n",
        "    print(f\"\\n{name}: {info['path'].name}\")\n",
        "    print(f\"   Path: {info['path']} ({'found' if exists else 'NOT FOUND'})\")\n",
        "    print(f\"   YAML: {info['yaml'].name} ({'found' if yaml_exists else 'NOT FOUND'})\")\n",
        "    print(f\"   Classes: {len(info['classes'])} - {info['classes']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5bSUfS58hnD"
      },
      "source": [
        "## **Load Fine-tuned Models for Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoMUptku8i2k"
      },
      "outputs": [],
      "source": [
        "def find_best_weights(model_dir):\n",
        "    \"\"\"Find the best.pt weights file in a model directory.\"\"\"\n",
        "    weights_path = model_dir / 'weights' / 'best.pt'\n",
        "    if weights_path.exists():\n",
        "        return weights_path\n",
        "    return None\n",
        "\n",
        "# Collect all trained models (6 total)\n",
        "FINETUNED_MODELS = {}\n",
        "\n",
        "model_runs = [\n",
        "    # Single dataset models\n",
        "    ('YOLO11n_RoadLane', MODELS_DIR / 'yolo11n_road_lane'),\n",
        "    ('YOLO11l_RoadLane', MODELS_DIR / 'yolo11l_road_lane'),\n",
        "    ('YOLO11n_BDD100K', MODELS_DIR / 'yolo11n_bdd100k'),\n",
        "    ('YOLO11l_BDD100K', MODELS_DIR / 'yolo11l_bdd100k'),\n",
        "    # Sequential training models (Road Lane -> BDD100K)\n",
        "    ('YOLO11n_Both', MODELS_DIR / 'yolo11n_both'),\n",
        "    ('YOLO11l_Both', MODELS_DIR / 'yolo11l_both'),\n",
        "]\n",
        "\n",
        "print(\"Loading Fine-tuned Models:\")\n",
        "print(\"=\" * 50)\n",
        "for name, model_dir in model_runs:\n",
        "    weights = find_best_weights(model_dir)\n",
        "    if weights:\n",
        "        try:\n",
        "            model = YOLO(weights)\n",
        "            FINETUNED_MODELS[name] = {\n",
        "                'model': model,\n",
        "                'path': weights,\n",
        "                'classes': list(model.names.values())\n",
        "            }\n",
        "            print(f\"{name}: Loaded ({len(model.names)} classes)\")\n",
        "        except Exception as e:\n",
        "            print(f\"{name}: Failed to load - {e}\")\n",
        "    else:\n",
        "        print(f\"{name}: Not found at {model_dir}\")\n",
        "\n",
        "print(f\"\\nTotal models loaded: {len(FINETUNED_MODELS)}/6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih_EneUU9Ofg"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained models for comparison\n",
        "PRETRAINED_MODELS = {\n",
        "    'YOLO11n_Pretrained': YOLO('yolo11n.pt'),\n",
        "    'YOLO11l_Pretrained': YOLO('yolo11l.pt'),\n",
        "}\n",
        "\n",
        "print(\"Pre-trained models (COCO):\")\n",
        "for name, model in PRETRAINED_MODELS.items():\n",
        "    print(f\"   {name}: {len(model.names)} classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4RhC_xS9XpJ"
      },
      "source": [
        "## **Performance Evaluation**\n",
        "\n",
        "Compare fine-tuned models against pre-trained models.\n",
        "\n",
        "### **Inference Speed Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nao0JeDW_L60"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(model, images, model_name, num_runs=3, warmup=2):\n",
        "    \"\"\"\n",
        "    Benchmark inference speed of a model.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    detections = []\n",
        "    confidences = []\n",
        "    class_counts = Counter()\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        if images:\n",
        "            _ = model(images[0], verbose=False)\n",
        "\n",
        "    # Benchmark\n",
        "    for _ in range(num_runs):\n",
        "        for img in images:\n",
        "            start = time.time()\n",
        "            results = model(img, verbose=False)\n",
        "            inference_time = (time.time() - start) * 1000\n",
        "            times.append(inference_time)\n",
        "\n",
        "            for r in results:\n",
        "                n_det = len(r.boxes)\n",
        "                detections.append(n_det)\n",
        "                if n_det > 0:\n",
        "                    confs = r.boxes.conf.cpu().numpy()\n",
        "                    confidences.extend(confs.tolist())\n",
        "                    for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
        "                        class_counts[model.names[cls_id]] += 1\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'avg_time_ms': np.mean(times),\n",
        "        'std_time_ms': np.std(times),\n",
        "        'fps': 1000 / np.mean(times),\n",
        "        'avg_detections': np.mean(detections),\n",
        "        'avg_confidence': np.mean(confidences) if confidences else 0,\n",
        "        'class_counts': dict(class_counts),\n",
        "        'total_detections': sum(class_counts.values())\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0a639l_x4L"
      },
      "outputs": [],
      "source": [
        "# Get test images from Road Lane dataset\n",
        "road_lane_test_dir = DATASETS['road_lane']['path'] / 'test' / 'images'\n",
        "test_images_lane = list(road_lane_test_dir.glob('*.jpg'))[:20] if road_lane_test_dir.exists() else []\n",
        "print(f\"Road Lane test images: {len(test_images_lane)}\")\n",
        "\n",
        "# Get validation images from BDD100K (new structure)\n",
        "bdd_val_dir = DATASETS['bdd100k']['path'] / 'valid' / 'images'\n",
        "test_images_bdd = list(bdd_val_dir.glob('*.jpg'))[:50] if bdd_val_dir.exists() else []\n",
        "print(f\"BDD100K validation images: {len(test_images_bdd)}\")\n",
        "\n",
        "# Combined test images\n",
        "all_test_images = test_images_lane[:10] + test_images_bdd[:10]\n",
        "print(f\"Combined test set: {len(all_test_images)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qGk0ZkN_kyS"
      },
      "outputs": [],
      "source": [
        "# Run benchmarks on Road Lane test set\n",
        "print(\"Benchmarking on Road Lane Test Set\")\n",
        "benchmark_results = {}\n",
        "\n",
        "if test_images_lane:\n",
        "    # Benchmark fine-tuned models\n",
        "    for name, info in FINETUNED_MODELS.items():\n",
        "        if 'RoadLane' in name:  # Only Road Lane models\n",
        "            print(f\"\\nBenchmarking {name}...\")\n",
        "            benchmark_results[name] = benchmark_inference(\n",
        "                info['model'], test_images_lane, name, num_runs=2\n",
        "            )\n",
        "\n",
        "    # Benchmark pre-trained models for comparison\n",
        "    for name, model in PRETRAINED_MODELS.items():\n",
        "        print(f\"\\nBenchmarking {name}...\")\n",
        "        benchmark_results[name] = benchmark_inference(\n",
        "            model, test_images_lane, name, num_runs=2\n",
        "        )\n",
        "else:\n",
        "    print(\"No Road Lane test images found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tGRfuvgRPjE"
      },
      "outputs": [],
      "source": [
        "# Display benchmark results\n",
        "if benchmark_results:\n",
        "    comparison_data = []\n",
        "    for name, result in benchmark_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': name,\n",
        "            'Avg Time (ms)': f\"{result['avg_time_ms']:.1f}\",\n",
        "            'FPS': f\"{result['fps']:.1f}\",\n",
        "            'Avg Detections': f\"{result['avg_detections']:.1f}\",\n",
        "            'Avg Confidence': f\"{result['avg_confidence']:.2%}\",\n",
        "            'Total Detections': result['total_detections']\n",
        "        })\n",
        "\n",
        "    df_results = pd.DataFrame(comparison_data)\n",
        "    print(df_results.to_string(index=False))\n",
        "else:\n",
        "    print(\"No benchmark results to display.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHgEEYc2SFKP"
      },
      "outputs": [],
      "source": [
        "if benchmark_results:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    models = list(benchmark_results.keys())\n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
        "\n",
        "    # Inference Time\n",
        "    ax = axes[0, 0]\n",
        "    times = [benchmark_results[m]['avg_time_ms'] for m in models]\n",
        "    bars = ax.bar(models, times, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Time (ms)')\n",
        "    ax.set_title('Inference Time (lower is better)', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    for bar, t in zip(bars, times):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{t:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # FPS\n",
        "    ax = axes[0, 1]\n",
        "    fps = [benchmark_results[m]['fps'] for m in models]\n",
        "    bars = ax.bar(models, fps, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('FPS')\n",
        "    ax.set_title('Frames Per Second (higher is better)', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    for bar, f in zip(bars, fps):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                f'{f:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Average Detections\n",
        "    ax = axes[1, 0]\n",
        "    dets = [benchmark_results[m]['avg_detections'] for m in models]\n",
        "    bars = ax.bar(models, dets, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Detections')\n",
        "    ax.set_title('Avg Detections per Image', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Average Confidence\n",
        "    ax = axes[1, 1]\n",
        "    confs = [benchmark_results[m]['avg_confidence'] for m in models]\n",
        "    bars = ax.bar(models, confs, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Confidence')\n",
        "    ax.set_title('Average Confidence Score', fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hClDS1ydS6fe"
      },
      "source": [
        "## **Detection Quality Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0irBasFwTC3v"
      },
      "outputs": [],
      "source": [
        "# Define traffic vs non-traffic classes for relevance analysis\n",
        "TRAFFIC_CLASSES = {\n",
        "    'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle',\n",
        "    'person', 'traffic light', 'stop sign', 'parking meter',\n",
        "    'lane', 'drivable area', 'road', 'rider', 'motor', 'bike',\n",
        "    'divider-line', 'dotted-line', 'double-line', 'random-line',\n",
        "    'road-sign-line', 'solid-line', 'traffic sign'\n",
        "}\n",
        "\n",
        "NON_TRAFFIC_CLASSES = {\n",
        "    'toothbrush', 'hair drier', 'wine glass', 'cup', 'fork', 'knife',\n",
        "    'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
        "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'couch', 'bed',\n",
        "    'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
        "    'baseball glove', 'skateboard', 'surfboard', 'tennis racket'\n",
        "}\n",
        "\n",
        "print(f\"Traffic-related classes: {len(TRAFFIC_CLASSES)}\")\n",
        "print(f\"Non-traffic classes: {len(NON_TRAFFIC_CLASSES)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wowJpoTSTQmf"
      },
      "outputs": [],
      "source": [
        "def analyze_detection_relevance(model, images, model_name):\n",
        "    \"\"\"\n",
        "    Analyze what classes the model detects and categorize them.\n",
        "    \"\"\"\n",
        "    traffic_detections = Counter()\n",
        "    non_traffic_detections = Counter()\n",
        "    other_detections = Counter()\n",
        "\n",
        "    for img in images:\n",
        "        results = model(img, verbose=False)\n",
        "        for r in results:\n",
        "            for cls_id in r.boxes.cls.cpu().numpy().astype(int):\n",
        "                class_name = model.names[cls_id]\n",
        "                if class_name.lower() in {c.lower() for c in TRAFFIC_CLASSES}:\n",
        "                    traffic_detections[class_name] += 1\n",
        "                elif class_name.lower() in {c.lower() for c in NON_TRAFFIC_CLASSES}:\n",
        "                    non_traffic_detections[class_name] += 1\n",
        "                else:\n",
        "                    other_detections[class_name] += 1\n",
        "\n",
        "    total = sum(traffic_detections.values()) + sum(non_traffic_detections.values()) + sum(other_detections.values())\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'traffic_detections': dict(traffic_detections),\n",
        "        'non_traffic_detections': dict(non_traffic_detections),\n",
        "        'other_detections': dict(other_detections),\n",
        "        'traffic_count': sum(traffic_detections.values()),\n",
        "        'non_traffic_count': sum(non_traffic_detections.values()),\n",
        "        'other_count': sum(other_detections.values()),\n",
        "        'total': total,\n",
        "        'traffic_ratio': sum(traffic_detections.values()) / max(total, 1)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVenncslUQ3x"
      },
      "outputs": [],
      "source": [
        "relevance_results = {}\n",
        "\n",
        "# Use combined test images for analysis\n",
        "test_images = all_test_images[:15] if all_test_images else test_images_lane[:15]\n",
        "\n",
        "if test_images:    \n",
        "    # Analyze fine-tuned models\n",
        "    for name, info in FINETUNED_MODELS.items():\n",
        "        print(f\"\\nAnalyzing {name}...\")\n",
        "        relevance_results[name] = analyze_detection_relevance(\n",
        "            info['model'], test_images, name\n",
        "        )\n",
        "\n",
        "    # Analyze pre-trained models\n",
        "    for name, model in PRETRAINED_MODELS.items():\n",
        "        print(f\"\\nAnalyzing {name}...\")\n",
        "        relevance_results[name] = analyze_detection_relevance(\n",
        "            model, test_images, name\n",
        "        )\n",
        "else:\n",
        "    print(\"No test images found for analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqXt6N3LUV0Y"
      },
      "outputs": [],
      "source": [
        "# Display relevance results\n",
        "for name, result in relevance_results.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"   Traffic-related detections: {result['traffic_count']}\")\n",
        "    print(f\"   Non-traffic detections: {result['non_traffic_count']}\")\n",
        "    print(f\"   Other detections: {result['other_count']}\")\n",
        "    print(f\"   Traffic Focus Ratio: {result['traffic_ratio']:.1%}\")\n",
        "\n",
        "    if result['traffic_detections']:\n",
        "        top_traffic = sorted(result['traffic_detections'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        print(f\"   Top traffic classes: {dict(top_traffic)}\")\n",
        "\n",
        "    if result['non_traffic_detections']:\n",
        "        print(f\"   Unwanted detections: {result['non_traffic_detections']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l05uXRatUdQA"
      },
      "outputs": [],
      "source": [
        "# Visualize detection relevance comparison\n",
        "if relevance_results:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    models = list(relevance_results.keys())\n",
        "\n",
        "    # Stacked bar chart: Traffic vs Non-Traffic detections\n",
        "    ax = axes[0]\n",
        "    traffic = [relevance_results[m]['traffic_count'] for m in models]\n",
        "    non_traffic = [relevance_results[m]['non_traffic_count'] for m in models]\n",
        "    other = [relevance_results[m]['other_count'] for m in models]\n",
        "\n",
        "    x = np.arange(len(models))\n",
        "    width = 0.6\n",
        "\n",
        "    ax.bar(x, traffic, width, label='Traffic', color='#2ecc71')\n",
        "    ax.bar(x, non_traffic, width, bottom=traffic, label='Non-Traffic', color='#e74c3c')\n",
        "    ax.bar(x, other, width, bottom=[t+n for t,n in zip(traffic, non_traffic)],\n",
        "           label='Other', color='#95a5a6')\n",
        "\n",
        "    ax.set_ylabel('Number of Detections')\n",
        "    ax.set_title('Detection Categories by Model', fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "\n",
        "    # Traffic Focus Ratio\n",
        "    ax = axes[1]\n",
        "    ratios = [relevance_results[m]['traffic_ratio'] * 100 for m in models]\n",
        "    colors = ['#2ecc71' if r > 80 else '#f39c12' if r > 50 else '#e74c3c' for r in ratios]\n",
        "    bars = ax.bar(models, ratios, color=colors, edgecolor='black')\n",
        "    ax.set_ylabel('Traffic Focus Ratio (%)')\n",
        "    ax.set_title('Traffic Detection Focus (higher = better)', fontweight='bold')\n",
        "    ax.set_ylim(0, 100)\n",
        "    ax.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='Good (80%)')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for bar, ratio in zip(bars, ratios):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
        "                f'{ratio:.0f}%', ha='center', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeH79GvIVETV"
      },
      "source": [
        "## **Visual Comparison: Pre-trained vs Fine-tuned**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E00sCNLJVJ9S"
      },
      "outputs": [],
      "source": [
        "def compare_detections(image_path, models_dict, title=\"Detection Comparison\"):\n",
        "    \"\"\"\n",
        "    Compare detection results from multiple models on the same image.\n",
        "    \"\"\"\n",
        "    n_models = len(models_dict)\n",
        "    fig, axes = plt.subplots(1, n_models + 1, figsize=(5 * (n_models + 1), 5))\n",
        "\n",
        "    # Original image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    axes[0].imshow(img_rgb)\n",
        "    axes[0].set_title('Original', fontsize=11)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Detection results from each model\n",
        "    for idx, (name, model) in enumerate(models_dict.items(), 1):\n",
        "        results = model(image_path, verbose=False)\n",
        "        annotated = results[0].plot()\n",
        "        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        n_det = len(results[0].boxes)\n",
        "\n",
        "        # Get detected classes\n",
        "        detected_classes = []\n",
        "        if n_det > 0:\n",
        "            for cls_id in results[0].boxes.cls.cpu().numpy().astype(int):\n",
        "                detected_classes.append(model.names[cls_id])\n",
        "\n",
        "        axes[idx].imshow(annotated_rgb)\n",
        "        axes[idx].set_title(f'{name}\\n({n_det} detections)', fontsize=10)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnj2J9z3VRVU"
      },
      "outputs": [],
      "source": [
        "# Compare models on sample images\n",
        "all_models = {}\n",
        "\n",
        "for name, info in FINETUNED_MODELS.items():\n",
        "    all_models[name] = info['model']\n",
        "\n",
        "all_models.update(PRETRAINED_MODELS)\n",
        "\n",
        "sample_images = all_test_images[:3] if all_test_images else test_images_lane[:3]\n",
        "\n",
        "if sample_images:\n",
        "    for i, img_path in enumerate(sample_images):\n",
        "        print(f\"\\nImage {i+1}: {img_path.name}\")\n",
        "        compare_detections(img_path, all_models, f\"Sample {i+1}: {img_path.name}\")\n",
        "else:\n",
        "    print(\"No sample images found for comparison!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-section"
      },
      "source": [
        "## **Summary & Conclusions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Oa7vevVU-w"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTRAINED MODELS\")\n",
        "for name, info in FINETUNED_MODELS.items():\n",
        "    print(f\"   {name}\")\n",
        "    print(f\"      Classes: {len(info['classes'])}\")\n",
        "    print(f\"      Path: {info['path']}\")\n",
        "\n",
        "print(\"\\nPERFORMANCE HIGHLIGHTS\")\n",
        "if benchmark_results:\n",
        "    fastest = min(benchmark_results.items(), key=lambda x: x[1]['avg_time_ms'])\n",
        "    most_detections = max(benchmark_results.items(), key=lambda x: x[1]['avg_detections'])\n",
        "\n",
        "    print(f\"   Fastest: {fastest[0]} ({fastest[1]['fps']:.1f} FPS)\")\n",
        "    print(f\"   Most detections: {most_detections[0]} ({most_detections[1]['avg_detections']:.1f} per image)\")\n",
        "s\n",
        "print(\"\\nDETECTION FOCUS\")\n",
        "if relevance_results:\n",
        "    for name, result in relevance_results.items():\n",
        "        status = \"Good\" if result['traffic_ratio'] > 0.8 else \"Mixed\" if result['traffic_ratio'] > 0.5 else \"Poor\"\n",
        "        print(f\"   {status} {name}: {result['traffic_ratio']:.1%} traffic-focused\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
